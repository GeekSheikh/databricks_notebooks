// Databricks notebook source
// MAGIC %md ## Example of Structured Streaming from Kafka source with Avro

// COMMAND ----------

val hosts = "ec2-52-34-7-209.us-west-2.compute.amazonaws.com:9092"//eg "host1:port1,host2:port2"
val topic = "USCISAvroDemo"
// val schemaRegistryUrl = "http://10.103.130.136:8081" //not used in this example

// COMMAND ----------

val df = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", hosts)
  .option("startingOffsets", "earliest")
  .option("subscribe", topic)
  .load()

// COMMAND ----------

// MAGIC %md _Note: using package to avoid serialization issues (cluster restart required if class is redefined)._

// COMMAND ----------

val masterSchemaString = """{
        "type": "record",
        "name": "state",
        "fields": [
            {
                "name": "id",
                "type": "int"
            },
            {
                "name": "st_cd",
                "type": "string"
            },
            {
                "name": "st_nm",
                "type": "string"
            },
            {
                "name": "modified",
                "type": {
                    "type": "long",
                    "connect.version": 1,
                    "connect.name": "org.apache.kafka.connect.data.Timestamp",
                    "logicalType": "timestamp-millis"
                }
            }
        ],
        "connect.name": "state"
    }"""
  
  val masterSchema = new Schema.Parser().parse(masterSchemaString)

// COMMAND ----------

package temp

case class MyRecord(id: Int, st_cd: String, st_nm: String, modified: Long)
case class USCIS_Record(
  id: Int,
  st_cd: String,
  st_nm: String,
  modified: Long)

// COMMAND ----------

import org.apache.avro.Schema
import org.apache.avro.generic.{GenericData, GenericDatumReader, GenericDatumWriter, GenericRecord}
import org.apache.avro.io.{Decoder, DecoderFactory, Encoder, EncoderFactory}
import kafkashaded.org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
import java.util.Properties

object MyDecoder {
  
  //schema provided by Sevatec
  //example record  {“id”: 1, “st_cd”: “MD”, “st_nm”: “Test Maryland”, “modified”: 1497969568798}
  val schema_string = """{
        "type": "record",
        "name": "state",
        "fields": [
            {
                "name": "id",
                "type": "int"
            },
            {
                "name": "st_cd",
                "type": "string"
            },
            {
                "name": "st_nm",
                "type": "string"
            },
            {
                "name": "modified",
                "type": {
                    "type": "long",
                    "connect.version": 1,
                    "connect.name": "org.apache.kafka.connect.data.Timestamp",
                    "logicalType": "timestamp-millis"
                }
            }
        ],
        "connect.name": "state"
    }"""
  val schema = new Schema.Parser().parse(schema_string)
  
  val reader = new GenericDatumReader[GenericRecord](schema)

  def decode(message: Array[Byte]): temp.USCIS_Record = {
    val decoder = DecoderFactory.get().binaryDecoder(message, null)
    val record = reader.read(null, decoder)
    
    temp.USCIS_Record(
      record.get("id").asInstanceOf[Int], 
      record.get("st_cd").toString, 
      record.get("st_nm").toString, 
      record.get("modified").asInstanceOf[Long]
    )
  }
}

// register a udf for decoding
val decodeRecord = org.apache.spark.sql.functions.udf { (message: Array[Byte]) => MyDecoder.decode(message) }

// COMMAND ----------

// MAGIC %md
// MAGIC #Producer

// COMMAND ----------

import kafka.javaapi.producer.Producer
import kafka.producer.ProducerConfig
import org.apache.avro.specific.SpecificDatumWriter
import java.io.ByteArrayOutputStream
import org.apache.avro.io._
import kafka.message.MessageAndMetadata
import org.apache.avro.Schema
import kafka.producer.KeyedMessage

// import org.apache.avro.generic.IndexedRecord
// import org.apache.avro.generic.GenericData
// import org.apache.avro.generic.GenericRecord
// import org.apache.avro.generic.GenericDatumReader
// import org.apache.avro.io.DatumReader
// import org.apache.avro.io.Decoder
// import org.apache.avro.io.DecoderFactory
// import org.apache.avro.specific.SpecificDatumReader
// import org.apache.commons.codec.DecoderException
// import org.apache.commons.codec.binary.Hex
// import kafka.consumer.ConsumerIterator
// import kafka.consumer.KafkaStream

// import java.io.File
// import java.io.IOException
// import java.nio.charset.Charset
// import java.util.Properties
// import org.apache.commons.codec.binary.Hex
object MyProducer {
  
  //schema provided by Sevatec
  //example record  {“id”: 1, “st_cd”: “MD”, “st_nm”: “Test Maryland”, “modified”: 1497969568798}
  
  def sendMessage(localSchema: Schema, payload: GenericRecord): Unit = {
    val props = new Properties()
    val topic = "USCISAvroDemo"

    props.put("metadata.broker.list", "ec2-52-34-7-209.us-west-2.compute.amazonaws.com:9092")
    props.put("message.send.max.retries", "5")
    props.put("request.required.acks", "-1")
    props.put("serializer.class", "kafka.serializer.DefaultEncoder")
  //   props.put("client.id", UUID.randomUUID().toString())

    val config = new ProducerConfig(props)
    val producer: Producer[String, Array[Byte]] = new Producer[String, Array[Byte]](config)
    val writer: DatumWriter[GenericRecord] = new SpecificDatumWriter[GenericRecord](localSchema)
    val out = new ByteArrayOutputStream()
    val encoder = EncoderFactory.get.binaryEncoder(out, null)
    writer.write(payload, encoder)
    encoder.flush()
    out.close()

    val serializedBytes = out.toByteArray()
    val message: KeyedMessage[String, Array[Byte]] = new KeyedMessage[String, Array[Byte]](topic, serializedBytes)
    producer.send(message)
    producer.close
  }
}

// COMMAND ----------

// MAGIC %md
// MAGIC ### Start the Stream

// COMMAND ----------

// Example-1: use as UDF
import org.apache.spark.sql.functions._
display(df.select('timestamp, decodeRecord($"value").as("record")).select('timestamp.alias("Kafka_Rec_Time"), $"record.*")
  .withColumn("tz_specific_kafkaTime", from_utc_timestamp('Kafka_Rec_Time, "US/Eastern")))

// COMMAND ----------

// MAGIC %md
// MAGIC ### Add new Avro Messages to Kafka
// MAGIC And watch them appear in the stream above

// COMMAND ----------

//example record  {“id”: 1, “st_cd”: “MD”, “st_nm”: “Test Maryland”, “modified”: 1497969568798}
val payload: GenericRecord = new GenericData.Record(masterSchema)
payload.put("id", 2)
payload.put("st_cd", "AL")
payload.put("st_nm", "Test Alabama")
payload.put("modified", "1497969557389".toLong)
MyProducer.sendMessage(masterSchema, payload)